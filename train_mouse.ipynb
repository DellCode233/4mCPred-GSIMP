{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from utils import get_performance, get_confusion_matrix\n",
    "\n",
    "from load_dataset import GetDataSet, iDataSet ,iFunction\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torchkeras import KerasModel\n",
    "import pandas as pd\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Accuracy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.correct = nn.Parameter(torch.tensor(0),requires_grad=False)\n",
    "        self.total = nn.Parameter(torch.tensor(0),requires_grad=False)\n",
    "        \n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "\n",
    "        matrix = get_confusion_matrix(F.softmax(preds, -1), targets) \n",
    "        TP ,FP ,FN ,TN = matrix[[1,1,0,0],[1,0,1,0]]  \n",
    "        correct_i = (TP + TN).long()\n",
    "        total_i = targets.numel()\n",
    "        self.correct += correct_i \n",
    "        self.total += total_i\n",
    "        return correct_i.float()/total_i\n",
    "    \n",
    "    def compute(self):\n",
    "        return self.correct.float()/self.total \n",
    "    \n",
    "    def reset(self):\n",
    "        self.correct-=self.correct\n",
    "        self.total-=self.total\n",
    "\n",
    "class Recall(nn.Module):\n",
    "    'Recall for binary-classification task'\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.true_positive = nn.Parameter(torch.tensor(0),requires_grad=False)\n",
    "        self.total_positive = nn.Parameter(torch.tensor(0),requires_grad=False)\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        y_pred = torch.softmax(preds, -1)[:,1].reshape(-1)\n",
    "        y_true = targets.reshape(-1)\n",
    "        assert y_pred.shape == y_true.shape\n",
    "        \n",
    "        true_positive_i = torch.sum((y_pred>=0.5)*(y_true>=0.5))\n",
    "        total_positive_i = torch.sum(y_true>=0.5)\n",
    "        self.true_positive += true_positive_i\n",
    "        self.total_positive += total_positive_i\n",
    "        return torch.true_divide(true_positive_i,total_positive_i)\n",
    "\n",
    "    def compute(self):\n",
    "        return torch.true_divide(self.true_positive,self.total_positive) \n",
    "    \n",
    "    def reset(self):\n",
    "        self.true_positive -= self.true_positive\n",
    "        self.total_positive -= self.total_positive\n",
    "\n",
    "class Mcc(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.matrix = nn.Parameter(torch.zeros(2,2),requires_grad=False)\n",
    "\n",
    "\n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        y_pred = preds.argmax(axis=1).type(targets.dtype).reshape(-1)\n",
    "        y_true = targets.reshape(-1)\n",
    "        self.matrix[0,0] += (y_pred | y_true == 0).sum()\n",
    "        self.matrix[1,1] += (y_pred & y_true).sum()\n",
    "        self.matrix[1,0] += (y_pred - y_true == 1).sum()\n",
    "        self.matrix[0,1] += (y_true - y_pred == 1).sum()        \n",
    "        TP ,FP ,FN ,TN = self.matrix[[1,1,0,0],[1,0,1,0]]\n",
    "        MCC = ((TP * TN) - (FP * FN)) / torch.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + 1e-06)\n",
    "        return MCC\n",
    "        \n",
    "    def compute(self):\n",
    "        TP ,FP ,FN ,TN = self.matrix[[1,1,0,0],[1,0,1,0]]\n",
    "        MCC = ((TP * TN) - (FP * FN)) / torch.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) + 1e-06)\n",
    "        return MCC\n",
    "    \n",
    "    def reset(self):\n",
    "        self.matrix -= self.matrix\n",
    "\n",
    "class AUC(nn.Module):\n",
    "    'approximate AUC calculation for binary-classification task'\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tp = nn.Parameter(torch.zeros(10001),requires_grad=False)\n",
    "        self.fp = nn.Parameter(torch.zeros(10001),requires_grad=False)\n",
    "        \n",
    "    def eval_auc(self,tp,fp):\n",
    "        tp_total = torch.sum(tp)\n",
    "        fp_total = torch.sum(fp)\n",
    "        length = len(tp)\n",
    "        tp_reverse = tp[range(length-1,-1,-1)]\n",
    "        tp_reverse_cum = torch.cumsum(tp_reverse,axis=0)-tp_reverse/2.0\n",
    "        fp_reverse = fp[range(length-1,-1,-1)]\n",
    "        \n",
    "        auc = torch.sum(torch.true_divide(tp_reverse_cum,tp_total)\n",
    "                        *torch.true_divide(fp_reverse,fp_total))\n",
    "        return auc\n",
    "        \n",
    "    def forward(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        y_pred = (10000*torch.softmax(preds, -1)[:,1]).reshape(-1).type(torch.int)\n",
    "        y_true = targets.reshape(-1)\n",
    "        \n",
    "        tpi = self.tp-self.tp\n",
    "        fpi = self.fp-self.fp\n",
    "        assert y_pred.shape == y_true.shape\n",
    "        for i,label in enumerate(y_true):\n",
    "            if label>=0.5:\n",
    "                tpi[y_pred[i]]+=1.0\n",
    "            else:\n",
    "                fpi[y_pred[i]]+=1.0\n",
    "        self.tp+=tpi\n",
    "        self.fp+=fpi\n",
    "        return self.eval_auc(tpi,fpi)\n",
    "          \n",
    "    def compute(self):\n",
    "        return self.eval_auc(self.tp,self.fp)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tp-=self.tp\n",
    "        self.fp-=self.fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_encoding = iFunction.fe1\n",
    "path = os.path.join(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_trial(lr, embedding_dim, batchsize):\n",
    "    dataset_name =  \"mouse\"\n",
    "    datasets = GetDataSet(os.path.join(path, dataset_name), iFunction.read_txt_to_pd2, dataset_name)\n",
    "    test_data = datasets(feature_encoding,1)\n",
    "    train_data = datasets(feature_encoding)\n",
    "    metric_dict = {\"Mcc\":Mcc()}\n",
    "    train_iter = data.DataLoader(train_data,batchsize,True)\n",
    "    valid_iter = data.DataLoader(test_data,len(test_data)) \n",
    "    net = classifier(4,embedding_dim,True).to(device)\n",
    "    net2 = classifier2(embedding_dim,True).to(device)\n",
    "    target_net = nn.Sequential(net,net2).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    opti = torch.optim.Adam(target_net.parameters(),lr)\n",
    "    model = KerasModel(target_net,loss_fn,optimizer=opti,metrics_dict=metric_dict)\n",
    "    model.fit(train_iter,valid_iter,200,patience=40, monitor='val_Mcc', mode=\"max\")\n",
    "    with torch.no_grad():\n",
    "        target_net.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "        X, y = next(iter(valid_iter))\n",
    "        X, y = X.to(device),y.to(device)\n",
    "        y_hat = target_net(X)\n",
    "        perfor = get_performance(y_hat.softmax(-1), y)\n",
    "        \n",
    "    return perfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kfold_valid(num_folds, batchsize, embedding_dim, lr):\n",
    "    all_perfors = []\n",
    "    kfd = KFold(num_folds,shuffle=True)\n",
    "    dataset_name =  \"mouse\"\n",
    "    datasets = GetDataSet(os.path.join(path, dataset_name), iFunction.read_txt_to_pd2, dataset_name)\n",
    "    train_pos, train_neg = datasets(feature_encoding, 0)\n",
    "    for ifold,(pos,neg) in enumerate(zip(kfd.split(train_pos),kfd.split(train_neg))):\n",
    "        metric_dict = {\"Mcc\":Mcc()}\n",
    "        train_set,valid_set = data.Subset(train_pos,pos[0]),data.Subset(train_pos,pos[1])\n",
    "        train_set += data.Subset(train_neg,neg[0])\n",
    "        valid_set += data.Subset(train_neg,neg[1])\n",
    "        train_iter = data.DataLoader(train_set,batchsize,True)\n",
    "        valid_iter = data.DataLoader(valid_set,len(valid_set)) \n",
    "        net = classifier(4,embedding_dim,True).to(device)\n",
    "        net2 = classifier2(embedding_dim,True).to(device)\n",
    "        target_net = nn.Sequential(net,net2).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "        opti = torch.optim.Adam(target_net.parameters(),lr)\n",
    "        model = KerasModel(target_net, loss_fn, optimizer=opti, metrics_dict=metric_dict)\n",
    "        model.fit(train_iter, valid_iter, epochs=200, patience=40, monitor='val_Mcc', mode=\"max\", quiet=True, plot=False)\n",
    "        with torch.no_grad():\n",
    "            target_net.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
    "            X, y = next(iter(valid_iter))\n",
    "            X, y = X.to(device),y.to(device)\n",
    "            y_hat = target_net(X)\n",
    "            perfor = get_performance(y_hat.softmax(-1), y)\n",
    "        all_perfors.append(pd.DataFrame.from_dict(perfor, orient='index').T)\n",
    "    return pd.concat(all_perfors, ignore_index=True).mean(axis=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
